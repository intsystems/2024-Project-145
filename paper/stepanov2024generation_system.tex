\documentclass{article}
% Required package
\usepackage{amsmath}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Создание персонализированных генераций изображений}

\author{Степанов Илья Дмитриевич \\
	\texttt{iliatut94@gmail.com} \\
	%% examples of more authors
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
        В генеративных моделях существует широкий спектр проблем, однако одной из наиболее актуальных является сложность создания высококачественных изображений конкретных людей с точностью, передающей их уникальную идентичность. Предлагается сфокусировать внимание на разработке моделей, способных генерировать изображения заданного человека в разнообразных вариациях и с высоким разрешением.Требуется обучить методы IP-Adapter на модели Stable Diffusion с использованием множественных подсказок в виде картинок.
\end{abstract}


\keywords{IP-Adapter\cite{1} \and Stable Diffusion \cite{2}}

\section{Введение}

В современных исследованиях активно развивается модель генерации изображений под названием Stable Diffusion.
Основная идея заключается в том, что на каждой итерации шум добавляется к текущему изображению, а затем происходит процесс диффузии, который плавно размывает этот шум, превращая его в сглаженное изображение.
Эта модель позволяет создавать высококачественные изображения на основе текстовых и графических подсказок, открывая новые возможности в области синтеза изображений. Однако, в процессе работы с моделями генерации изображений, в том числе и Stable Diffusion, возникают определенные проблемы, одна из них --- недостаточное соответствие сгенерированных изображений исходным текстовым подсказкам.

Для решения указанной проблемы предложен метод IP-Adapter, который представляет собой простой способ адаптации изображений к текстовым подсказкам с использованием cross-attention. Cross-attention --- это метод, применяемый в моделях генерации изображений для улучшения взаимосвязей между различными частями изображения. В данном случае он интегрируется в существующие диффузионные модели, что способствует повышению точности генерации изображений.

Моя мотивация заключается в повышении качества визуальных представлений за счет более точного соответствия текстовым и графическим подсказкам.

Целью моего исследования является разработка метода на основе IP-Adapter. Мотивация заключается в повышении качества визуальных представлений за счет более точного соответствия текстовым и графическим подсказкам.В модели принимается набор изображений и соответствующих текстовых подсказок.При обучении модели одно изображение удаляется равновероятно, и модель учится восстанавливать это удаленное изображение, опираясь на текстовое описание и другие имеющиеся изображения. В исследовании также предлагается варьирование количества изображений.Точность результатов отслеживается с помощью метрик качества генераций изображений.

Для основы исследования я использую статью IP-Adapter и Latent Diffusion, а также датасет CELEBa в качестве исходных данных.
Основными метриками качества будут FID, IS.

\section{Постановка задачи}

Обычная диффузионная модель минимизирует данную функцию потерь:

\begin{equation}
L_{\text{simple}} = \mathbb{E}_{x_0, \epsilon \sim N(0, I), c, t} ||\epsilon - \hat{\epsilon}_{\theta}(x_t, c, t) ||^2 
\end{equation}

где \(x_0\) представляет собой исходное изображение с дополнительным условием \(c\), \(t \in [0, T]\) обозначает шаг времени диффузионного процесса, \(x_t = \alpha_t x_0 + \sigma_t \epsilon\) --- шумные данные на \(t\)-м шаге, а \(\alpha_t\), \(\sigma_t\) --- заранее определенные функции от \(t\), определяющие процесс диффузии.

\begin{equation}
\hat{\epsilon}_{\theta}(x_t, c, t) = w\epsilon_{\theta}(x_t, c, t) + (1 - w)\epsilon_{\theta}(x_t, t)
\end{equation}

здесь \(w\) является нормировочной константой, которая регулирует соответствие условию \(c\). Для моделей диффузии такой выбор \(\hat{\epsilon}_{\theta}(x_t, c, t)\) играет ключевую роль в улучшении соответствия изображения тексту сгенерированных образцов.

С помощью IP-Adapter признаки изображения интегрируются в заранее обученную модель UNet с помощью cross-attention. В оригинальной модели Stable Diffusion закодированный текст подается в модель UNet через слои перекрестного внимания. Учитывая признаки запроса \(Z\) и признаки текста \(c_t\), выход перекрестного внимания \(Z'\) может быть определен следующим уравнением:

\begin{equation}
Z' = \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\end{equation}

где Q, K и V - матрицы запроса, ключа и значений операции внимания, соответственно, а \(W_q\), \(W_k\), \(W_v\) --- матрицы весов обучаемых слоев линейной проекции, переменная \(d\) играет роль нормирующей константы.


Модифицированный IP-адаптер обучается на наборе данных \(X = \{(c_i, t_i) : i = 1, \ldots, N\}\), где \(c_i\) --- входное изображение, \(t_i\) --- соответствующая ему текстовая подсказка. Задача минимизировать данную функцию потерь:

\begin{equation}
   L_{simple} = \underset{\hat\epsilon_{\theta}}{\arg\min}\ \mathbb{E}_{x_0, \epsilon \sim N(0, I), c, c_{1}, \ldots c_{N}, t} ||\epsilon - \hat{\epsilon}_{\theta}(x_t, c, c_{1}, \ldots c_{N}, t_{1} \ldots t_{N}) ||^2 
\end{equation}

Также используется нормировочное условие на изображения и текст в процессе обучения:

\begin{equation}
\hat{\epsilon_{\theta}}(x_{t}, c_{t}, c_{1}, \ldots c_{N}, t_{1},  \ldots t_{N}) = w \epsilon_{\theta}(x_{t}, c_{t}, c_{1}, \ldots c_{N}, t_{1} , \ldots t_{N}) + (1 - w) \epsilon_{\theta}(x_{t}, t)
\end{equation}

Я использую структуру для cross-attention к изображениям, аналогичную текстовому cross-attention. Следовательно, необходимо добавить \(2N\) параметров вида \(W_k_p'\) и \(W_v_p'\) для каждого слоя UNet, где \(N\) - количество изображений. Для ускорения сходимости параметры \(W_k_p'\) и \(W_v_p'\) инициализируются из \(W_k\) и \(W_v\). Затем мы используем агрегирующие функции \(G\)(\(K_{1}'\), \(K_{2}'\), \ldots \(K_{N}'\)) и \(U\)(\(V_{1}'\), \(V_{2}'\), \ldots \(V_{N}'\)) для получения конечной структуры cross-attention:

Поскольку мы замораживаем оригинальную модель UNet, только \(W_k_p'\) и \(W_v_p'\) обучаемы в вышеупомянутом методе.
На стадии вывода \(\lambda\) является нормировочным коэффицентом, который помогает настраивать вес условия изображения:

\begin{equation}
 Z'' = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V + \lambda \cdot \text{Softmax}\left(\frac{QG^T}{\sqrt{d}}\right)U
\end{equation}

где Q = \(ZW_q\), K = \(c_tW_k\), V = \(c_tW_v\), \(K_p'\) = \(c_pW_k_p'\),  \(V_p'\) = \(c_pW_v_p'\)

Стоит заметить, что модель становится оригинальной моделью распространения текста в изображение, если $\lambda = 0$.

Метрики качества:

Frechet Inception Distance (FID), Inception Score (IS) --- это метрики качества, которые используются для оценки качества сгенерированных изображений

Формула для FID:

\begin{equation}
FID = ||\mu_p - \mu_q||^2 + Tr(\Sigma_p + \Sigma_q - 2(\Sigma_p\Sigma_q)^{1/2})
\end{equation}

где \( \mu_p \) и \( \mu_q \) --- средние значения признаков в реальных и сгенерированных изображениях соответственно, \( \Sigma_p \) и \( \Sigma_q \) --- ковариационные матрицы для распределений признаков в реальных и сгенерированных изображениях соответственно.

Формула для IS:

\begin{equation}
IS(x) = \exp(\mathbb{E}_x \left[ D_{KL}(p(y | x) || p(y)) \right] )
\end{equation}

Где \( D_{KL} \) - дивергенция Кульбака-Лейблера для двух распределений \(p(y|x)\) - вероятность класса \(y\) для изображения \(x\) и \(p(y)\) - равномерное распределение на множестве классов, \( \mathbb{E}_x \) - математическое ожидание по всем изображениям \(x\).

\begin{thebibliography}{1}
\bibitem{1}"IP-Adapter" 
\url{https://arxiv.org/pdf/2308.06721.pdf}.

\bibitem{2}"Latent Stable Diffusion" 
\url{https://arxiv.org/abs/2112.10752.pdf}.
\end{thebibliography}

\end{document}
