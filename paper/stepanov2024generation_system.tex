\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Создание персонализированных генераций изображений}

\author{Степанов Илья Дмитриевич \\
	\texttt{iliatut94@gmail.com} \\
	%% examples of more authors
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
        В генеративных моделях существует широкий спектр проблем, однако одной из наиболее актуальных является сложность создания высококачественных изображений конкретных людей с точностью, передающей их уникальную идентичность. Предлагается сфокусировать внимание на разработке моделей, способных генерировать изображения заданного человека в разнообразных вариациях и с высоким разрешением.Требуется обучить методы IP-Adapter на модели Stable Diffusion с использованием множественных подсказок в виде картинок.
\end{abstract}


\keywords{IP-Adapter \and Stable Diffusion}

\section{Введение}

В современных исследованиях активно развивается модель генерации изображений под названием Stable Diffusion. Эта модель позволяет создавать высококачественные изображения на основе текстовых и графических подсказок, открывая новые возможности в области синтеза изображений. Однако, в процессе работы с моделями генерации изображений, в том числе и Stable Diffusion, возникают определенные проблемы, такие как недостаточное соответствие сгенерированных изображений исходным подсказкам.

Для решения этой проблемы был предложен метод IP-Adapter, который представляет собой легкий способ адаптации изображений к текстовым подсказкам с использованием стратегии кросс-внимания. Этот метод внедряется в существующие текстово-изображенческие диффузионные модели, что помогает улучшить точность генерации изображений.

Целью моего исследования является разработка метода на основе IP-Adapter для улучшения точности генерации изображений путем увеличения объема обучающих данных. Моя мотивация заключается в повышении качества визуальных представлений за счет более точного соответствия текстовым и графическим подсказкам.

Объектом моего исследования будет метод IP-Adapter в контексте модели Stable Diffusion. Для основы исследования я планирую использовать статью о IP-Adapter и Latent Diffusion, а также датасет CELEBa в качестве исходных данных.
\section{Постановка задачи}

В основном задача сводится к минимизации функции потерь в дифузионной модели:

\[
L_{\text{simple}} = \mathbb{E}_{x_0, \epsilon \sim N(0, I), c, t} ||\epsilon - \hat{\epsilon}_{\theta}(x_t, c, t) ||^2 
\]

где \(x_0\) представляет собой исходное изображение с дополнительным условием \(c\), \(t \in [0, T]\) обозначает шаг времени диффузионного процесса, \(x_t = \alpha_t x_0 + \sigma_t \epsilon\) - шумные данные на \(t\)-м шаге, а \(\alpha_t\), \(\sigma_t\) - заранее определенные функции от \(t\), определяющие процесс диффузии.

\[
\hat{\epsilon}_{\theta}(x_t, c, t) = w\epsilon_{\theta}(x_t, c, t) + (1 - w)\epsilon_{\theta}(x_t, t), (2)
\]

здесь \(w\) является нормировочной константой, которая регулирует соответствие условию \(c\). Для моделей диффузии такой выбор \(\hat{\epsilon}_{\theta}(x_t, c, t)\) играет ключевую роль в улучшении соответствия изображения тексту сгенерированных образцов.

Признаки изображения интегрируются в заранее обученную модель \(UNet\) с помощью cross-attention.В оригинальной модели Stable Diffusion признаки закодированного текста подаются в модель \(UNet\) через слои перекрестного внимания. Учитывая признаки запроса \(Z\) и признаки текста \(c_t\), выход перекрестного внимания \(Z'\) может быть определен следующим уравнением:

\[ Z' = \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\]


где Q, K и V - матрицы запроса, ключа и значений операции внимания, соответственно, а \(W_q\), \(W_k\), \(W_v\) - матрицы весов обучаемых слоев линейной проекции, переменная \(d\) играет роль нормирующей константы.

Я использую ту же структуру для cross-attention к изображениям, что и для текстового cross-attention. Следовательно, нам необходимо добавить \(2N\) параметров \(W_k_p'\) и \(W_v_p'\) для каждого слоя cross-attention, где \(N\) - количество изображений. Для ускорения сходимости параметры \(W_k_p'\) и \(W_v_p'\) инициализируются из \(W_k\) и \(W_v\). Затем мы используем агрегирующие функции \(G\)(\(K_1'\), \(K_2'\), ... \(K_N'\)) 
и \(U\)(\(V_1'\), \(V_2'\), ... \(V_N'\)) чтобы получить конечную структуру cross-attention:

\[ Z'' = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V + \text{Softmax}\left(\frac{QG^T}{\sqrt{d}}\right)U \]

где Q = \(ZW_q\), K = \(c_tW_k\), V = \(c_tW_v\), \(K_p'\) = \(c_iW_k_p'\),  \(V_p'\) = \(c_iW_v_p'\)

Поскольку мы замораживаем оригинальную модель UNet, только \(W_k_p'\) и \(W_v_p'\) обучаемы в вышеприведенном отделенном перекрестном внимании.

Во время обучения мы оптимизируем только IP-адаптер, оставляя параметры предобученной модели дифузии фиксированными. IP-адаптер также обучается на наборе данных с изображениями и текстом, используя ту же целевую функцию обучения, что и оригинальный SD:

\[
L_{simple} = E_{x_{0},\epsilon,c_{t},c_{i},t} \left\| \epsilon - \epsilon_{\theta}(x_{t},c_{t},c_{i},t) \right\|^2
\]

Мы также случайным образом нормируем условие изображения на стадии обучения:

\[
\hat{\epsilon_{\theta}}(x_{t}, c_{t}, c_{i}, t) = w \epsilon_{\theta}(x_{t}, c_{t}, c_{i}, t) + (1 - w) \epsilon_{\theta}(x_{t}, t). (7)
\]

Здесь мы просто обнуляем внедрение изображения CLIP, если условие изображения отбрасывается.
Так как текстовое взаимное внимание и взаимное внимание к изображению отсоединены, мы также можем настраивать вес условия изображения на стадии вывода:

\[
Z_{new} = Attention(Q,K,V) + \lambda \cdot Attention(Q,K',V')
\]

где $\lambda$ - это весовой коэффициент, и модель становится оригинальной моделью распространения текста в изображение, если $\lambda = 0$.

\end{document}
