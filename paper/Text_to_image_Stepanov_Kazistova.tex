\documentclass{article}
% Required package
\usepackage{amsmath}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Создание персонализированных генераций изображений}

\author{ Кристина М.~Казистова \\
	ФПМИ\\
	МФТИ\\
	Долгопрудный \\
	\texttt{kazistova.km@phystech.edu} \\
	%% examples of more authors
	\And
	Степанов Илья Дмитриевич \\
	ФПМИ\\
	МФТИ\\
	Долгопрудный \\
	\texttt{iliatut94@gmail.com} \\
    \And
	Филатов Андрей Викторович \\
	Сколковский Институт Технологий\\
	Москва \\
	\texttt{filatovandreiv@gmail.com} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}


\renewcommand{\undertitle}{}
\renewcommand{\headeright}{}
\renewcommand{\shorttitle}{\textit{Создание персонализированных генераций изображений}}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf

\begin{document}
\maketitle

\begin{Аннотация}
        Модели генерации изображений совершили значительный скачок в области искусственного интеллекта, обеспечив высококачественный и разнообразный синтез изображений из заданного текстового описания. Однако, когда возникает запрос на генерацию специфичного объекта, в нашем случае человека, модель не может сгенерировать его с необходимой точностью и передать его идентичность.Предлагается решение, которое будет способно генерировать изображения заданного человека в различных вариациях в высоком разрешении.Мы представляем подход, в основе которого лежит метод IP-Adapter'a. Данный подход способен обрабатывать несколько изображений одновременно, что приводит к повышению качества генерации.
\end{Аннотация}

\textbf{Ключевые слова}: Диффузионная модель, Stable Diffusion\cite{2}, IP-Adapter\cite{1}, DreamBooth\cite{3}.

\section{Введение}В последние годы наблюдается быстрое развитие генеративных моделей, которые решают задачу генерации изображений. Существующие модели способны генерировать разнообразные изображения по текстовым описаниям с высокой точностью. Однако, в процессе работы с моделями генерации изображений возникают определенные проблемы, одной из которых является недостаточное соответствие сгенерированных изображений к исходным текстовым подсказкам. Наша задача заключается в повышении качества визуальных представлений за счет большего количества изображений. В работе рассматриваются методы, которые позволяют решить вышеупомянутые проблемы, и затем сравниваются между собой. Все описанные далее подходы основаны на применении дифузионной модели\cite{2}. 

Эта модель состоит из двух процессов: прямого и обратного.Во время прямого процесса ко входным данным постепенно добавляется шум, а во время обратного процесса модель постепенно восстанавливает данные из шума. Эта модель позволяет создавать высококачественные изображения на основе текстовых и графических подсказок, открывая новые возможности в области синтеза изображений.

Первый представленный метод --- это DreamBooth\cite{3}.Он принимает на вход несколько изображений одного объекта вместе с соответствующим названием класса и возвращает специальный токен, идентифицирующий объект, который затем встраивается в текстовое описание, по которой генерируется желаемое изображение. Проблемы данного метода заключаются в слабой адаптивности, отсутствии обобщения и необходимости обучать всю диффузионную модель. 

Второй метод --- это IP-Adapter\cite{1}. Он состоит из двух частей: энкодера для извлечения признаков изображения, текста и адаптированных модулей с механизмом перекрестного внимания. Метод принимает на вход только одно изображение объекта. Однако одной картинки может быть мало, для того чтобы модель могла уловить все необходимые зависимости. 

В работе предлагается третий метод, представляющий собой модификацию IP-Adapter. На вход подаются несколько изображений вместо одного, причем каждому изображению соответствует своя текстовая подсказка. В процессе обучения модели одно изображение удаляется равновероятно, и модель учится восстанавливать это удаленное изображение, опираясь на текстовое описание и другие имеющиеся изображения.К латентным представлениям изображений применяется агрегирующая функция. За счет подачи нескольких изображений добиваемся лучшей передачи идентичности. 
Рассмотренные методы сравниваются между собой по метрикам качества генерации и разнообразия, метрикам идентичности.
Исследование проводится на выборке из датасета LFW Deep Funneled\cite{5} --- датасете изображений знаменитостей в высоком разрешении. 

\section{Постановка задачи}

Определим датасет как $ \mathfrak{D}=\{ ({x}_{i}, \mathbf{\tau}_{i}): i = 1, \dots, n\}$, ${x}_{i}$ --- изображение, $\mathbf{\tau}_{i}$ -- текстовая подсказка. На этапе обучения на каждом шаге из $\mathfrak{D}$ удаляется изображение ${x}_{j}, j \sim \mathcal{U}\{1, \dots, n\}$.

Определим функцию потерь: 
\begin{equation}
\mathcal{L}(\epsilon, \epsilon_{\theta}) = \mathbb{E}_{\epsilon \sim N(0, I),\mathbf{c}_{\tau}, G(\mathbf{c}_{i} \textbackslash \{\mathbf{c}^{j}\}), t, \mathbf{c}^{j}_{t}} \|\epsilon - \epsilon_{\theta}(\mathbf{c}_{\tau}, G(\mathbf{c}_{i} \textbackslash \{\mathbf{c}^{j}\}), t, \mathbf{c}^{j}_{t})\|^2,
\end{equation}
где $G$ --- агрегирующая функция, применяемая ко входным данным; $\mathbf{c}_{\tau}$ --- текстовые признаки удаленного изображения; $\mathbf{c}_{i}$ --- признаки изображений; $\mathbf{c}^{j}$ --- признаки удаленного изображения; $t \in [0, T]$ --- временной шаг диффузионного процесса; $\mathbf{c}^{j}_{t} = \alpha_t \mathbf{c}^{j}  + \sigma_t \epsilon$ --- зашумленные данные удаленного изображения на шаге $t$; $\alpha_t, \sigma_t$ --- предопределенные функции от $t$, определяющие диффузионный процесс; $\epsilon_{\theta}$ --- предсказание шума.

Решается следующая оптимизационная задача:

\begin{equation}
\epsilon_{\theta}^* = \arg \min_{\epsilon_{\theta}}\mathcal{L}(\epsilon, \epsilon_{\theta}),
\end{equation}

Следует отметить, что в основе IP-Adapter лежит алгоритм CrossAttention. Сам алгоритм на вход принимает признаки изображения и 

Поскольку перекрестное внимание к тексту и перекрестное внимание к изображению разделены, мы также можем настроить вес условия изображения на этапе вывода:  
\begin{equation}
\mathbf{Z}^{new} = Attention(\mathbf{Q}, \mathbf{K}, \mathbf{V}) + \lambda \cdot Attention(\mathbf{Q}, \mathbf{K}^{'}, \mathbf{V}^{'}), 
\end{equation}
где $\lambda$ --- весовой коэффициент, $\mathbf{Z}$ --- признаки запроса, $\mathbf{Q} = \mathbf{Z}\mathbf{W}_q$, $\mathbf{K} = \mathbf{c}_t\mathbf{W}_k, \mathbf{K}^{'} = G(\mathbf{c}_i \textbackslash \{\mathbf{c}^j\}) {\mathbf{W}}^{'}_{k}, \mathbf{V} = \mathbf{c}_t\mathbf{W}_{v}, G(\mathbf{c}_i \textbackslash \{\mathbf{c}^j\}){\mathbf{W}}^{'}_{v}$ --- матрицы запросов, ключей и значений механизмов внимания для текста и изображений соответственно, a $\mathbf{W}_q, \mathbf{W}_k, {\mathbf{W}}^{'}_{k_i}, \mathbf{W}_v, {\mathbf{W}}^{'}_{v_i}, {\mathbf{W}}^{'}_k_j,{\mathbf{W}}^{'}_v_j$ --- соответствующие весовые матрицы. 

Метрики качества:

Frechet Inception Distance (FID), Inception Score (IS) --- это метрики качества, которые используются для оценки качества сгенерированных изображений
\begin{equation}
FID = ||\mu_p - \mu_q||^2 + Tr(\mathbf{\Sigma_p} + \mathbf{\Sigma_q} - 2(\mathbf{\Sigma_p}\mathbf{\Sigma_q})^{1/2})
\end{equation}

где \( \mu_p \) и \( \mu_q \) --- средние значения признаков в реальных и сгенерированных изображениях соответственно, \( \mathbf{\Sigma_p} \) и \( \mathbf{\Sigma_q} \) --- ковариационные матрицы для распределений признаков на реальных и сгенерированных изображениях соответственно.
\begin{equation}
IS(x) = \exp(\mathbb{E}_x \left[ D_{KL}(p(y | x) || p(y)) \right] )
\end{equation}

Где \( D_{KL} \) - дивергенция Кульбака-Лейблера для двух распределений \(p(y|x)\) - вероятность класса \(y\) для изображения \(x\) и \(p(y)\) - равномерное распределение на множестве классов.

\section{Метод}
В данной секции мы сначала введем необходимые понятия, затем опишем принцип работы существующих методов решения поставленной задачи: IP-Adapter и DreamBooth. Наконец, представим описание разработанного нами метода.
\subsection{Вводные сведения}
\subsubsection{Диффузионные модели}
Диффузионная модель состоит из двух процессов: прямого и обратного.

Прямой процесс представляет собой последовательность зашумленных версий входного изображения $X_0, \dots, X_T$, где $T$ --- количество шагов, a $X_t$ получается по следующей формуле: $$X_t = \sqrt{1-\beta_t}X_{t-1}+\sqrt{\beta_t}\varepsilon,$$ где $\varepsilon \sim \mathcal{N}(0, I)$, $$X_t|X_{t-1} \sim \mathcal{N}(\sqrt{1-\beta_t}X_{t-1}, \beta_t I).$$ При $T \rightarrow \infty, X_T \rightarrow \mathcal{N}(0, I).$ На последнем шаге итераций получается гауссовский шум. 

Положим $\alpha_t=1-\beta_t, \overline{\alpha_t} = \prod\limits_{s = 1}^t\alpha_s$. Тогда $$X_t = \sqrt{\overline{\alpha_t}}X_0+\sqrt{1-\overline{\alpha_t}}\varepsilon,$$ где $\varepsilon \sim \mathcal{N}(0, I)$, $$X_t|X_0 \sim \mathcal{N}(\sqrt{\overline{\alpha_t}}X_0, (1-\overline{\alpha_t}) I).$$

Во время обратного процесса исходное изображение восстанавливается из шума. Знаем $X_T\sim \mathcal{N}(0, I)$. Семплирование происходит итеративно шаг за шагом: $$\hat{X}_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left( \hat{X}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha_t}}}\hat{\varepsilon}_t\right) + \sqrt{\beta_t}\varepsilon,$$ где $\hat{X}_t$ --- восстановленное изображение на итерации $t$, при этом, если $t = T$, то $\hat{X}_t = X_t$; $\hat{\varepsilon}_t$ --- реконструкция шума, полученная моделью для $\hat{X}_t$; $\varepsilon \sim \mathcal{N}(0, I)$ --- шум, который позволяет генерировать различные изображения. 

\subsubsection{CLIP}
CLIP --- это мультимодальная модель, обученная методом контрастивного обучения на большом наборе данных, содержащем пары изображение-текст. 

\subsubsection{Stable Diffusion}
Процесс работы Stable Diffusion состоит из трех основных этапов. Сначала текстовый энкодер преобразует текст в эмбеддинги. Затем диффузионная модель генерирует изображения в латентном пространстве, используя эмбеддинги. Наконец, VAE декодер переводит сгенерированное изображение в исходное пространство. В качестве текстового энкодера используется текстовый энкодер предобученной модели CLIP. Для модели диффузии используется UNet архитектура с механизмом внимания, который объединяет между собой латентные представления изображений с текстовыми эмбеддингами. ResNet блоки данной архитектуры напрямую не взаимодействуют с текстовыми эмбеддингами. 

\subsubsection{Classifier Guidance}
Чтобы явно включить информацию о классе в процесс диффузии, можно обучить классификатор $f_\phi(c|x_t, t)$ на зашумленном изображении $x_t$ и использовать градиенты $\Delta_x \log f_\phi(c|x_t)$, чтобы направлять процесс диффузионного семплирования к информации об условии $c$ путем изменения прогнозирования шума. Добавим вес $w$, который будет усиливать сlassifier guidance, и теперь модель для предсказания шума будет выглядеть так: $$\hat{\epsilon}_\theta(x_t, t) = \epsilon_\theta(x_t, t) - \sqrt{1 - \overline{\alpha}_t} w \Delta_{x_{t}}\log f_\phi(c|x_t)$$

\subsubsection{Classifier-free Guidance}
Можно регулировать силу влияния условия $c$ без специального классификатора. Метод classifier-free guidance позволяет увеличить степень, с которой модель ориентируется на промпт. Во время семплирования предсказание получается путем линейной комбинации предсказаний обусловленной и необусловленной моделей: $$\hat{\epsilon}_{\theta}(x_t, c, t) = w\epsilon_{\theta}(x_t, c, t) + (1-w)\epsilon_{\theta}(x_t, t)$$

\section{Планирование эксперимента}
В эксперименте рассматривается задача генерации изображений с помощью существующих моделей DreamBooth, IP-Adapter, а также модификации IP-Adapter на датасете LFW Deep Funneled.\cite{5}

\subsection{DreamBooth}
Как уже отмечалось ранее, данная модель принимает на вход несколько изображений одного объекта вместе с соответствующим названием класса и возвращает специальный токен, идентифицирующий объект. Затем этот токен встраивается в текстовую подсказку, по которой генерируется желаемое изображение. Параллельно применяется функция потерь сохранения класса, основанная на семантическом контексте модели относительно класса, что стимулирует генерацию разнообразных экземпляров, принадлежащих классу субъекта. Вычисление метрик $FID$ и $IS$ производится на всем датасете.\par
Определим функцию потерь для модели DreamBooth:
\begin{equation}
\mathcal{L}(\epsilon, \epsilon_{\theta}, \epsilon^{pr}_{\theta}) = \mathbb{E}_{\epsilon \sim N(0, I),\mathbf{c}_{\tau}, t, \mathbf{x}_{t}} \|\epsilon - \epsilon_{\theta}(\mathbf{c}_{\tau}, t, \mathbf{x}_{t})\|^2 + \lambda \cdot \mathbb{E}_{\epsilon^{pr} \sim N(0, I),\mathbf{c}^{pr}_{\tau}, t, \mathbf{x}^{pr}_{t}} \|\epsilon^{pr} - \epsilon_{\theta}(\mathbf{c}^{pr}_{\tau}, t, \mathbf{x}^{pr}_{t})\|^2,
\end{equation}
$\mathbf{c}_{\tau}$ --- текстовые признаки изображений с токеном; $\mathbf{c}^{pr}_{\tau}$ --- текстовые признаки класса изображений; $t \in [0, T]$ --- временной шаг диффузионного процесса; $\mathbf{x}_{t} = \alpha_t \mathbf{x}  + \sigma_t \epsilon$ --- зашумленные данные изображения на шаге $t$; $\mathbf{x}^{pr}_{t} = \alpha_t^{pr} \mathbf{x}^{pr}  + \sigma_t^{pr} \epsilon^{pr}$ --- зашумленные данные класса изображений на шаге $t$; $\alpha_t, \sigma_t$, $\alpha^{pr}_t, \sigma^{pr}_t$ --- предопределенные функции от $t$, определяющие диффузионный процесс; $\epsilon_{\theta}$, $\epsilon^{pr}_{\theta}$ --- цели обучения модели диффузии; $\lambda$ --- 
весовой коэффицент.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Dream.jpeg}
    \caption{DreamBooth}
    \label{fig:simple}
\end{figure}

\subsection{IP-Adapter}
IP-Adapter состоит из двух основных компонентов: энкодера, который извлекает признаки изображения и текста, и модулей адаптации с механизмом перекрестного внимания. Он принимает на вход одно изображение. По сравнению с моделью DreamBooth, IP-Adapter обладает большей адаптивностью. Данный подход включает свои модули в предварительно обученную диффузионную модель, что позволяет обучать только энкодер и механизм перекрестного внимания. Вычисление метрик $FID$ и $IS$ производится на всем датасете. \par
Определим функцию потерь для модели IP-Adapter:
\begin{equation}
\mathcal{L}(\epsilon, \epsilon_{\theta}) = \mathbb{E}_{\epsilon \sim N(0, I),\mathbf{c}_{\tau}, \mathbf{c}_{i}, t, \mathbf{x}_{t}} \|\epsilon - \epsilon_{\theta}(\mathbf{c}_{\tau}, \mathbf{c}_{i} , t, \mathbf{x}_{t})\|^2,
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Simple.jpeg}
    \caption{IP-Adapter}
    \label{fig:simple}
\end{figure}


\subsection{IP-AdapterMAX и IP-AdapterAVG}
Данная модификация метода IP-Adapter включает в себя обработку нескольких изображений, к которым применяются аггрегирующие функции MAXpooling или AVGpooling для их латентных представлений. На вход подаются изображения людей, вычисляются эмбеддинги данных изображений, после чего к эмбеддингам применяются упомянутые ранее функции аггрегации. В данном случае полученное латентное представление интегрируется в полностью предобученную модель IP-Adapter. Вычисление метрик производится на всем датасете.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Pooling.jpeg}
    \caption{IP-Adapter with Pooling}
    \label{fig:pooling}
\end{figure}


\subsection{IP-AdapterSelf-Attention}
Предложенная модификация метода IP-Adapter включает в себя обработку нескольких изображений, к которым применяется алгоритм Self-Attention\cite{4} для их латентных представлений. Исходный датасет разделяется на тренировочную и тестовую выборки в соотношении $2:1$. Тренировочная выборка содержит набор персон, каждая из которых обладает 10 изображениями, к каждому из которых прилагается текстовая подсказка. Обучение происходит на 9 изображениях: в ходе обучения осуществляется попытка предсказать 10-е изображение, используя текстовую подсказку и предварительно обработанные эмбеддинги 9 изображений. Выбор удаленного изображения осуществляется равновероятно.

\begin{algorithm}
\caption{Self-Attention}
\label{self_attention}
\begin{algorithmic}
\Procedure{Self-Attention}{$\mathbf{x}$}
    \State $\mathbf{Q} \gets \mathbf{x} \cdot \mathbf{W}_{q}$
    \State $\mathbf{K} \gets \mathbf{x} \cdot \mathbf{W}_{k}$
    \State $\mathbf{V} \gets \mathbf{x} \cdot \mathbf{W}_{v}$
    \State $\mathbf{Z} \gets \text{$softmax$}\left(\frac{\mathbf{Q} \cdot \mathbf{K}^T}{\sqrt{d_k}}\right) \cdot \mathbf{V}$
    \State \Return $\mathbf{Z} \cdot \mathbf{W}_{out}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

После завершения этапа модуля Self-Attention последуют модули IP-Adapter без изменений. В данном случае обучаются модули Self-Attention, Linear и Cross-Attention. Поскольку модификация Self-Attention обучается на 9 изображениях, то если от пользователя поступит большее или меньшее число изображений, в первом случае лишние изображения просто удаляются, а во втором выполняется процедура бутстрэпа до достижения нужного количества картинок.


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{SelfAttention.jpeg}
    \caption{IP-Adapter with Self-Attention}
    \label{fig:self_attention}
\end{figure}


\section{Результаты эксперимента}

\begin{table}[h]
\begin{tabular}{l c c c c}
\toprule
\textbf{Метод} & \textbf{IS} & \textbf{FID}\\
\midrule
IP-Adapter & 15.37  & 8.92\\
DreamBooth & 17.64 & 9.61\\
IP-AdapterMAX & 14.12 & 10.10\\
IP-AdapterAVG & 13.56 & 11.82\\
IP-AdapterSelf-Attention & 18.72 & 7.56\\
\bottomrule
\end{tabular}
\end{table}

\section{Заключение}
Модели преобразования текста в изображение подтолкнули вперед возможности Искусственного Интеллекта, позволяя создавать качественные и разнообразные изображения на основе текстовых описаний. Тем не менее, возникают трудности при создании изображений конкретных объектов, таких как люди, из-за ограничений точности и передачи идентичности. Для преодоления этих проблем предлагаются новые решения, в том числе улучшенная модификация IP-Адаптера, которая умеет обрабатывать несколько изображений одновременно и улучшает качество генерации.Работа рассматривает методы DreamBooth, IP-Adapter, а также предлагает улучшенную модификацию IP-Адаптера, которая умеет обрабатывать несколько изображений одновременно и улучшает качество генерации.В качестве аггрегирующей функции были рассмотрены Pooling и механизм SelfAttention.Данные методы показали высокие значения на метриках качества FID и IS.В дальнейшем существует возможность модифировать уже наши алгоритмы посредством использования LoRA\cite{7}, FaceNet\cite{6}  и других.

\begin{thebibliography}{1}
\bibitem{1}"IP-Adapter" 
\url{https://arxiv.org/pdf/2308.06721.pdf}.

\bibitem{2}"Latent Stable Diffusion" 
\url{https://arxiv.org/abs/2112.10752.pdf}.

\bibitem{3}"DreamBooth" 
\url{https://arxiv.org/pdf/2208.12242.pdf}.

\bibitem{4}"Attention" 
\url{https://arxiv.org/pdf/1706.03762.pdf}.

\bibitem{5}"Dataset" 
\url{https://vis-www.cs.umass.edu/lfw/}.

\bibitem{6}"FaceNet" 
\url{https://arxiv.org/abs/1503.03832}.

\bibitem{7}"LoRA" 
\url{https://arxiv.org/pdf/2106.09685}.

\end{thebibliography}

\end{document}
