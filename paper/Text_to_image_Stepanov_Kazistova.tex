\documentclass{article}
% Required package
\usepackage{amsmath}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Создание персонализированных генераций изображений}

\author{ Кристина М.~Казистова \\
	ФПМИ\\
	МФТИ\\
	Долгопрудный \\
	\texttt{kazistova.km@phystech.edu} \\
	%% examples of more authors
	\And
	Степанов Илья Дмитриевич \\
	ФПМИ\\
	МФТИ\\
	Долгопрудный \\
	\texttt{iliatut94@gmail.com} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}



\renewcommand{\shorttitle}{\textit{Создание персонализированных генераций изображений}}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{Аннотация}
        В генеративных моделях одной из наиболее актуальных задач является сложность создания высококачественных изображений конкретных людей с точностью, передающей их уникальную идентичность.Предлагается сфокусировать внимание на разработке моделей, способных генерировать изображения заданного человека в разнообразных вариациях и с высоким разрешением.Требуется обучить различные модификации метода IP-Adapter на модели Stable Diffusion с использованием множественных подсказок в виде картинок.
\end{Аннотация}


\keywords{IP-Adapter\cite{1} \and Stable Diffusion \cite{2}}

\section{Введение}В последние годы наблюдается быстрое развитие генеративных моделей, которые решают задачу преобразования текста в изображение. Существующие модели способны генерировать разнообразные изображения по текстовым описаниям с высокой точностью. Однако, в процессе работы с моделями генерации изображений возникают определенные проблемы, одной из которых является недостаточное соответствие сгенерированных изображений и исходным текстовым подсказкам. Наша задача заключается в повышении качества визуальных представлений за счет большего количества графических подсказок. В работе рассматриваются методы, которые позволяют решить вышеупомянутые проблемы, и затем сравниваются между собой. Все описанные далее подходы основаны на применении Stable Diffusion\cite{2}. Диффузионная модель представляет собой модель, состоящую из двух процессов: прямого и обратного. Во время прямого процесса к входным данным постепенно добавляется шум, а во время обратного процесса модель постепенно восстанавливает данные из шума. Эта модель позволяет создавать высококачественные изображения на основе текстовых и графических подсказок, открывая новые возможности в области синтеза изображений.

Первый представленный метод --- это DreamBooth\cite{3}.Он принимает на вход несколько изображений одного объекта вместе с соответствующим названием класса и возвращает специальный токен, идентифицирующий объект, который затем встраивается в текстовую подсказку, по которой генерируется желаемое изображение. Проблемы данного метода заключаются в слабой адаптивности, отсутствии обобщения и необходимости обучать всю диффузионную модель. 

Второй метод --- это IP-Adapter\cite{1}. Он состоит из двух частей: энкодера для извлечения признаков изображения, текста и адаптированных модулей с механизмом перекрестного внимания. Метод принимает на вход только одно изображение объекта. Однако одной картинки может быть мало, для того чтобы модель могла уловить все необходимые зависимости. 

В работе предлагается третий метод, представляющий собой модификацию IP-Adapter. На вход подаются несколько изображений вместо одного, причем каждому изображению соответствует своя текстовая подсказка. В процессе обучения модели одно изображение удаляется равновероятно, и модель учится восстанавливать это удаленное изображение, опираясь на текстовое описание и другие имеющиеся изображения. К этим имеющимся изображениям применяется агрегирующая функция. За счет подачи нескольких изображений добиваемся лучшей передачи идентичности. 
Рассмотренные методы сравниваются между собой по критериям качества генерации и разнообразия, критериям идентичности.
Исследование проводится на выборке из датасета LFW Deep Funneled --- датасете изображений знаменитостей в высоком разрешении. 

\section{Постановка задачи}

Определим датасет как $ \mathfrak{D}=\{ (\mathbf{x}_{i}, \mathbf{\tau}_{i}): i = 1, \dots, n\}$, $\mathbf{x}_{i}$ --- латентное представление изображения, $\mathbf{\tau}_{i}$ -- текстовая подсказка. На этапе обучения на каждом шаге из $\mathfrak{D}$ удаляется изображение $\mathbf{x}_{j}, j \sim \mathcal{U}\{1, \dots, n\}$ и решается следующая оптимизационная задача:

\begin{equation}
\epsilon_{\theta}^* = \arg \min_{\epsilon_{\theta}}\mathcal{L}(\epsilon, \epsilon_{\theta}),
\end{equation}

Определим функцию потерь: 
\begin{equation}
\mathcal{L}(\epsilon, \epsilon_{\theta}) = \mathbb{E}_{\epsilon \sim N(0, I),\mathbf{c}_{\tau}, G(\mathbf{c}_{i} \textbackslash \{\mathbf{c}^{j}\}), t, \mathbf{x}^{j}_{t}} \|\epsilon - \epsilon_{\theta}(\mathbf{c}_{\tau}, G(\mathbf{c}_{i} \textbackslash \{\mathbf{c}^{j}\}), t, \mathbf{x}^{j}_{t})\|^2,
\end{equation}
где $G$ --- агрегирующая функция, применяемая ко входным данным; $\mathbf{c}_{\tau}$ --- текстовые признаки удаленного изображения; $\mathbf{c}_{i}$ --- признаки изображений; $\mathbf{c}^{j}$ --- признаки удаленного изображения; $t \in [0, T]$ --- временной шаг диффузионного процесса; $\mathbf{x}^{j}_{t} = \alpha_t \mathbf{x}^{j}  + \sigma_t \epsilon$ --- зашумленные данные удаленного изображения на шаге $t$; $\alpha_t, \sigma_t$ --- предопределенные функции от $t$, определяющие диффузионный процесс; $\epsilon_{\theta}$ --- цель обучения модели диффузии.

Также в данной работе регулируется условие изображения с помощью константы \(w\), чтобы обеспечить управление без использования классификатора на этапе вывода. 
\begin{equation}
\hat{\epsilon}_{\theta}(\mathbf{c}_{\tau}, G(\mathbf{c}_{i} \textbackslash \{\mathbf{c}^{j}\}), t, \mathbf{x}^{j}_{t}) = w\epsilon_{\theta}(\mathbf{c}_{\tau}, G(\mathbf{c}_{i} \textbackslash \{\mathbf{c}^{j}\}), t, \mathbf{x}^{j}_{t}) + (1-w)\epsilon_{\theta}(\mathbf{c}_{\tau}, t, \mathbf{x}^{j}_{t})
\end{equation}

Поскольку перекрестное внимание к тексту и перекрестное внимание к изображению разделены, мы также можем настроить вес условия изображения на этапе вывода:  
\begin{equation}
\mathbf{Z}^{new} = Attention(\mathbf{Q}, \mathbf{K}, \mathbf{V}) + \lambda \cdot Attention(\mathbf{Q}, \mathbf{K}^{'}, \mathbf{V}^{'}), 
\end{equation}
где $\lambda$ --- весовой коэффициент, $\mathbf{Z}$ --- признаки запроса, $\mathbf{Q} = \mathbf{Z}\mathbf{W}_q$, $\mathbf{K} = \mathbf{c}_t\mathbf{W}_k, \mathbf{K}^{'} = G(\mathbf{c}_i \textbackslash \{\mathbf{c}^j\}) {\mathbf{W}}^{'}_{k}, \mathbf{V} = \mathbf{c}_t\mathbf{W}_{v}, G(\mathbf{c}_i \textbackslash \{\mathbf{c}^j\}){\mathbf{W}}^{'}_{v}$ --- матрицы запросов, ключей и значений механизмов внимания для текста и изображений соответственно, a $\mathbf{W}_q, \mathbf{W}_k, {\mathbf{W}}^{'}_{k_i}, \mathbf{W}_v, {\mathbf{W}}^{'}_{v_i}, {\mathbf{W}}^{'}_k_j,{\mathbf{W}}^{'}_v_j$ --- соответствующие весовые матрицы. 

Метрики качества:

Frechet Inception Distance (FID), Inception Score (IS) --- это метрики качества, которые используются для оценки качества сгенерированных изображений
\begin{equation}
FID = ||\mu_p - \mu_q||^2 + Tr(\mathbf{\Sigma_p} + \mathbf{\Sigma_q} - 2(\mathbf{\Sigma_p}\mathbf{\Sigma_q})^{1/2})
\end{equation}

где \( \mu_p \) и \( \mu_q \) --- средние значения признаков в реальных и сгенерированных изображениях соответственно, \( \mathbf{\Sigma_p} \) и \( \mathbf{\Sigma_q} \) --- ковариационные матрицы для распределений признаков на реальных и сгенерированных изображениях соответственно.
\begin{equation}
IS(x) = \exp(\mathbb{E}_x \left[ D_{KL}(p(y | x) || p(y)) \right] )
\end{equation}

Где \( D_{KL} \) - дивергенция Кульбака-Лейблера для двух распределений \(p(y|x)\) - вероятность класса \(y\) для изображения \(x\) и \(p(y)\) - равномерное распределение на множестве классов, \( \mathbb{E}_x \) - математическое ожидание по всем изображениям \(x\).

\section{Планирование эксперимента}
В эксперименте рассматривается задача генерации изображений с помощью существующих моделей DreamBooth, IP-Adapter, а также модификации IP-Adapter на датасете LFW Deep Funneled.

\subsection{DreamBooth}
Как уже отмечалось ранее, данная модель принимает на вход несколько изображений одного объекта вместе с соответствующим названием класса и возвращает специальный токен, идентифицирующий объект. Затем этот токен встраивается в текстовую подсказку, по которой генерируется желаемое изображение. Параллельно применяется функция потерь сохранения класса, основанная на семантическом контексте модели относительно класса, что стимулирует генерацию разнообразных экземпляров, принадлежащих классу субъекта. Вычисление метрик $FID$ и $IS$ производится на всем датасете.\par
Определим функцию потерь для модели DreamBooth:
\begin{equation}
\mathcal{L}(\epsilon, \epsilon_{\theta}, \epsilon^{pr}_{\theta}) = \mathbb{E}_{\epsilon \sim N(0, I),\mathbf{c}_{\tau}, t, \mathbf{x}_{t}} \|\epsilon - \epsilon_{\theta}(\mathbf{c}_{\tau}, t, \mathbf{x}_{t})\|^2 + \lambda \cdot \mathbb{E}_{\epsilon^{pr} \sim N(0, I),\mathbf{c}^{pr}_{\tau}, t, \mathbf{x}^{pr}_{t}} \|\epsilon^{pr} - \epsilon_{\theta}(\mathbf{c}^{pr}_{\tau}, t, \mathbf{x}^{pr}_{t})\|^2,
\end{equation}
$\mathbf{c}_{\tau}$ --- текстовые признаки изображений с токеном; $\mathbf{c}^{pr}_{\tau}$ --- текстовые признаки класса изображений; $t \in [0, T]$ --- временной шаг диффузионного процесса; $\mathbf{x}_{t} = \alpha_t \mathbf{x}  + \sigma_t \epsilon$ --- зашумленные данные изображения на шаге $t$; $\mathbf{x}^{pr}_{t} = \alpha_t^{pr} \mathbf{x}^{pr}  + \sigma_t^{pr} \epsilon^{pr}$ --- зашумленные данные класса изображений на шаге $t$; $\alpha_t, \sigma_t$, $\alpha^{pr}_t, \sigma^{pr}_t$ --- предопределенные функции от $t$, определяющие диффузионный процесс; $\epsilon_{\theta}$, $\epsilon^{pr}_{\theta}$ --- цели обучения модели диффузии; $\lambda$ --- 
весовой коэффицент.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Dream.jpeg}
    \caption{DreamBooth}
    \label{fig:simple}
\end{figure}

\subsection{IP-Adapter}
IP-Adapter состоит из двух основных компонентов: энкодера, который извлекает признаки изображения и текста, и модулей адаптации с механизмом перекрестного внимания. Он принимает на вход одно изображение. По сравнению с моделью DreamBooth, IP-Adapter обладает большей адаптивностью. Данный подход включает свои модули в предварительно обученную диффузионную модель, что позволяет обучать только энкодер и механизм перекрестного внимания. Вычисление метрик $FID$ и $IS$ производится на всем датасете. \par
Определим функцию потерь для модели IP-Adapter:
\begin{equation}
\mathcal{L}(\epsilon, \epsilon_{\theta}) = \mathbb{E}_{\epsilon \sim N(0, I),\mathbf{c}_{\tau}, \mathbf{c}_{i}, t, \mathbf{x}_{t}} \|\epsilon - \epsilon_{\theta}(\mathbf{c}_{\tau}, \mathbf{c}_{i} , t, \mathbf{x}_{t})\|^2,
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Simple.jpeg}
    \caption{IP-Adapter}
    \label{fig:simple}
\end{figure}


\subsection{IP-AdapterMAX и IP-AdapterAVG}
Данная модификация метода IP-Adapter включает в себя обработку нескольких изображений, к которым применяются аггрегирующие функции MAXpooling или AVGpooling для их латентных представлений. На вход подаются изображения людей, вычисляются эмбеддинги данных изображений, после чего к эмбеддингам применяются упомянутые ранее функции аггрегации. В данном случае полученное латентное представление интегрируется в полностью предобученную модель IP-Adapter. Вычисление метрик производится на всем датасете.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Pooling.jpeg}
    \caption{IP-Adapter with Pooling}
    \label{fig:pooling}
\end{figure}


\subsection{IP-AdapterSelf-Attention}
Предложенная модификация метода IP-Adapter включает в себя обработку нескольких изображений, к которым применяется алгоритм Self-Attention\cite{4} для их латентных представлений. Исходный датасет разделяется на тренировочную и тестовую выборки в соотношении $2:1$. Тренировочная выборка содержит набор персон, каждая из которых обладает 10 изображениями, к каждому из которых прилагается текстовая подсказка. Обучение происходит на 9 изображениях: в ходе обучения осуществляется попытка предсказать 10-е изображение, используя текстовую подсказку и предварительно обработанные эмбеддинги 9 изображений. Выбор удаленного изображения осуществляется равновероятно.

\begin{algorithm}
\caption{Self-Attention}
\label{self_attention}
\begin{algorithmic}
\Procedure{Self-Attention}{$\mathbf{x}$}
    \State $\mathbf{Q} \gets \mathbf{x} \cdot \mathbf{W}_{q}$
    \State $\mathbf{K} \gets \mathbf{x} \cdot \mathbf{W}_{k}$
    \State $\mathbf{V} \gets \mathbf{x} \cdot \mathbf{W}_{v}$
    \State $\mathbf{Z} \gets \text{$softmax$}\left(\frac{\mathbf{Q} \cdot \mathbf{K}^T}{\sqrt{d_k}}\right) \cdot \mathbf{V}$
    \State \Return $\mathbf{Z} \cdot \mathbf{W}_{out}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

После завершения этапа модуля Self-Attention последуют модули IP-Adapter без изменений. В данном случае обучаются модули Self-Attention, Linear и Cross-Attention. Поскольку модификация Self-Attention обучается на 9 изображениях, то если от пользователя поступит большее или меньшее число изображений, в первом случае лишние изображения просто удаляются, а во втором выполняется процедура бутстрэпа до достижения нужного количества картинок.


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{SelfAttention.jpeg}
    \caption{IP-Adapter with Self-Attention}
    \label{fig:self_attention}
\end{figure}


\section{Результаты эксперимента}

\begin{table}[h]
\begin{tabular}{l c c c c}
\toprule
\textbf{Метод} & \textbf{IS} & \textbf{FID}\\
\midrule
IP-Adapter & 15.37  & 8.92\\
DreamBooth & 17.64 & 9.61\\
IP-AdapterMAX & 14.12 & 10.10\\
IP-AdapterAVG & - & -\\
IP-AdapterSelf-Attention & - & -\\
\bottomrule
\end{tabular}
\end{table}

\section{Заключение}

\begin{thebibliography}{1}
\bibitem{1}"IP-Adapter" 
\url{https://arxiv.org/pdf/2308.06721.pdf}.

\bibitem{2}"Latent Stable Diffusion" 
\url{https://arxiv.org/abs/2112.10752.pdf}.

\bibitem{3}"DreamBooth" 
\url{https://arxiv.org/pdf/2208.12242.pdf}.

\bibitem{4}"Attention" 
\url{https://arxiv.org/pdf/1706.03762.pdf}.

\end{thebibliography}

\end{document}
